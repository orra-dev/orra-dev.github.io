---
layout: default
---

# ðŸª¡ The orra Engineering Blog

Welcome to our engineering blog â€“ a collection of technical deep dives into the challenges we're solving while building [orra](https://github.com/orra-dev/orra).

Here we share the architectural decisions, implementation details, and hard-won lessons from building a production-grade orchestration system for LLM applications. From semantic caching to execution planning, from vector operations to state management, we document our journey through the technical complexities that arise when building reliable AI systems.

No fluff, just engineering insights for builders who appreciate the details. 

## Latest Posts

- [Self-hosting LLMs for Production Systems: Solving the Model Quality Challenge](/_posts/2025-04-07-self-hosting-llms-for-production-systems-solving-model-quality-challenge.md)

- [Semantic Caching for LLM Execution Plans: How We Cut Costs by 90%](/_posts/2025-03-17-semantic-caching-in-orra-plan-engine.md)
